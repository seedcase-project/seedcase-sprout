---
title: "Creating and managing data resources"
order: 2
jupyter: python3
---

In each [data package](/docs/design/interface/outputs.qmd#outputs) are
[data resources](/docs/design/interface/outputs.qmd#outputs), which
contain conceptually standalone sets of data. This page shows you how to
create and manage data resources inside a data package using Sprout. You
will need to have already [created a data
package](packages.qmd#creating-a-data-package).

{{< include _preamble.qmd >}}

::: callout-important
Data resources can only be created from [tidy
data](https://design.seedcase-project.org/data/). Before you can store
it, you need to process it into a tidy format, ideally using Python so
that you have a record of the steps taken to clean and transform the
data.
:::

Putting your data into a data package makes it easier for yourself and
others to use later on. The steps you'll take to get your data into the
structure used by Sprout are:

1.  Create the properties for the resource, using the original data as a
    starting point, and edit as needed.
2.  Create the folders for the new resource within the package and save
    the resource properties in the `datapackage.json` file.
3.  Add your data to the batches of data for the resource.
4.  Merge the data batches into a new resource data file.
5.  Re-build the data package's `README.md` file from the updated
    `datapackage.json` file.
6.  If you need to update the properties at a later point, you can use
    `update_resource_properties()` and then write the result to the
    `datapackage.json` file.

```{python setup}
#| include: false
# This `setup` code chunk creates a package and downloads some data.
import polars as pl
import seedcase_sprout.core as sp
from tempfile import mkdtemp
from pathlib import Path
from urllib.request import urlretrieve

temp_path = Path(mkdtemp())
package_path = sp.PackagePath(temp_path / "diabetes-study")
package_path.root().mkdir()

package_properties = sp.example_package_properties()
sp.write_package_properties(
    properties=package_properties,
    path=package_path.properties(),
)
readme = sp.as_readme_text(package_properties)
sp.write_file(readme, package_path.readme())

# TODO: Maybe eventually move this over into Sprout as an example dataset, rather than via a URL.
# Download the example data to a temporary location.
url = "https://raw.githubusercontent.com/seedcase-project/data/refs/heads/main/patients/patients.csv"
raw_data_path = temp_path / "patients.csv"
urlretrieve(
    url,
    raw_data_path
)

data = pl.read_csv(raw_data_path)
```

Making a data resource requires that you have data that can be made into
a resource in the first place. Usually, generated or collected data
starts out in a bit of a "raw" shape that needs some working. This work
needs to be done before adding the data as a data package, since Sprout
assumes that the data is already
[tidy](https://design.seedcase-project.org/data/). For this guide, we
use (fake) data that is already tidy and loaded into a Polars DataFrame:

```{python}
data.head(5)
```

If you want to follow this guide with the same data, you can find it
[here](https://raw.githubusercontent.com/seedcase-project/data/refs/heads/main/patients/patients.csv).

Before we start, you need to import Sprout as well as other helper
packages:

```{python}
import seedcase_sprout.core as sp

# For pretty printing of output
from pprint import pprint

# TODO: This could be a wrapper helper function instead
# To be able to write multiline strings without indentation
from textwrap import dedent
```

## Extracting resource properties from the data

You'll start by creating the resource's properties. Before you can store
data in your data package, you need to describe it using properties
(i.e., metadata). The resource's properties are what allow other people
to understand what your data is about and to use it more easily. These
properties also define what it means for data in the resource to be
correct, as all data in the resource must match the properties. While
you can create a resource properties object manually using
`ResourceProperties`, it can be quite intensive and time-consuming if
you, for example, have many columns in your data. To make this process
easier, `extract_resource_properties()` allows you to create an initial
resource properties object by extracting as much information as possible
from the Polars DataFrame with your data. Afterwards, you can edit these
properties as needed.

Now you are ready to use `extract_resource_properties()` to extract the
resource properties from the data. This function tries to infer the data
types from the data, but it might not get it right, so make sure to
double check the output. It is not possible to infer information that is
not included in the data, like a description of what the data contains
or the unit of the data.

```{python}
resource_properties = sp.extract_resource_properties(
    data=data
)
print(resource_properties)
```

<!-- TODO: pprint doesn't work well with nested objects like resource_properties above.
So, for now, we'll use print here. but, we should find an alternative.
 -->

You may be able to see that some things are missing, for instance, the
individual columns (called `fields`) don't have any descriptions. You
will have to manually add this yourself. You can run a check on the
properties to confirm what is missing:

```{python}
#| error: true
print(sp.check_resource_properties(resource_properties))
```

Time to fill in the missing fields of the resource properties:

```{python}
# TODO: Need to consider/design how editing can be done in an easier, user-friendly way.
# TODO: Add more detail when we know what can and can't be extracted.
resource_properties.name = "patient-data"
resource_properties.title = "Patient Data"
resource_properties.description = "This data resource contains data about..."
```

<!-- TODO: The last required field `Path` is still missing at this point -->

## Creating a data resource

Now that you have the properties for the resource, you can create the
resource itself within your existing data package. As a data package can
contain multiple resources, each resource is stored in a separate
folder. So in this step you will make a folder for the new resource
within the data package and save the resource properties to the
`datapackage.json` file of the package.

We assume that you've already created a data package (for example, by
following the [package guide](packages.qmd#creating-a-data-package)) and
stored the path to it as a `PackagePath` object in the `package_path`
variable. In this guide, the root folder of the package is in a
temporary folder:

```{python}
print(package_path.root())
```

Let's take a look at the current files and folders in the data package:

```{python}
#| echo: false
print(list(package_path.root().glob("**/*")))
```

This shows that the data package already includes a `datapackage.json`
file and a `README.md` file.

The next step is to write the resource properties to the
`datapackage.json` file. Before they are added, they will be checked to
confirm that they are correctly filled in and that no required fields
are missing. You can use the `PackagePath().properties()` helper
function to give you the location of the `datapackage.json` file based
on the path to your package.

```{python}
sp.write_resource_properties(
    resource_properties=resource_properties,
    path=package_path.properties(),
)
```

Let's check the contents of the `datapackage.json` file to see that the
resource properties have been added:

```{python}
print(sp.read_properties(package_path.properties()))
```

<!-- TODO: pprint doesn't work well with nested objects like resource_properties above.
So, for now, we'll use print here. but, we should find an alternative.
 -->

## Storing a backup of the data as a batch file

::: callout-note
See the [flow diagrams](/docs/design/interface/flows.qmd#flows) for a
simplified flow of steps involved in adding batch files.
:::

As shown above, the data is currently stored loaded as a Polars
DataFrame called `data`. Now, it's time to store this data in the
resource's folder by using:

```{python}
#| eval: false
# TODO: eval when function implemented
sp.write_resource_batch(
    data=...,
    resource_properties=resource_properties
)
```

This function uses the properties object to determine where to store the
data as a batch file, which is in the `batch/` folder of the resource's
folder. You can check the newly added file by using:

```{python}
print(package_path.resource_batch_files(1))
```

## Building the resource data file

Now that you've stored the data as a batch file, you can build the
Parquet file that will be used as the data resource. This Parquet file
is built from all the data in the `batch/` folder. Since there is only
one batch data file stored in the resource's folder, only this one will
be used to build the data resource's Parquet file:

```{python}
#| eval: false
# TODO: eval when function implemented
sp.join_resource_batches(
    data_list=...,
    resource_properties=resource_properties
)
```

::: callout-tip
If you add more data to the resource later on, you can update this
Parquet file to include all data in the batch folder using the
`build_resource_parquet()` function like shown above.
:::

## Re-building the README file

One of the last steps to adding a new data resource is to re-build the
`README.md` file of the data package. To allow some flexibility with
what gets added to the README text, this next function will only *build
the text*, but not write it to the file. This allows you to add
additional information to the README text before writing it to the file.

```{python}
readme_text = sp.as_readme_text(
    properties=sp.read_properties(package_path.properties())
)
```

For this guide, you'll only use the default text and not add anything
else to it. Next you write the text to the `README.md` file by:

```{python}
sp.write_file(
    string=readme_text,
    path=package_path.readme()
)
```

## Updating resource properties

After having created a resource, you may need to make edits to the
properties. While technically you can do this manually by opening up the
`datapackage.json` file and editing it, we strongly recommend you use
the update functions to do this. These functions help to ensure that the
`datapackage.json` file is still in a correct JSON format and has the
correct fields filled in. You can call the
`update_resource_properties()` function with two arguments: the current
resource properties and a resource properties object representing the
updates you want to make. This latter object will very often be a
partial resource properties, in the sense that it will have only those
fields filled in that you want to update. The function will return an
updated resource properties object. Any field in the object representing
the updates will overwrite the corresponding field in the current
properties object.

```{python}
#| eval: false
# TODO: eval when function implemented
resource_properties = sp.update_resource_properties(
    current_properties=resource_properties,
    update_properties=sp.ResourceProperties(
        title="Basic characteristics of patients"
    )
)
pprint(resource_properties)
```

Finally, to write your changes back to `datapackage.json`, use the
`write_resource_properties()` function:

```{python}
sp.write_resource_properties(
    resource_properties=resource_properties,
    path=package_path.properties(),
)
```

```{python}
#| include: false
import shutil

if temp_path.exists():
    shutil.rmtree(temp_path)
```
